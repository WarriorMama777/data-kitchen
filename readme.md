
- [â–¼æ—¥æœ¬èª](#æ—¥æœ¬èª)
- [data-kitchenğŸ³](#data-kitchen)
  - [æ¦‚è¦](#æ¦‚è¦)
  - [ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã«ã¤ã„ã¦](#ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã«ã¤ã„ã¦)
  - [ä¸»ãªæ©Ÿèƒ½](#ä¸»ãªæ©Ÿèƒ½)
  - [ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«](#ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«)
    - [å¿…è¦ãªç’°å¢ƒ](#å¿…è¦ãªç’°å¢ƒ)
    - [ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ‰‹é †](#ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ‰‹é †)
  - [ä½¿ã„æ–¹](#ä½¿ã„æ–¹)
    - [image\_converter\_pillow.py](#image_converter_pillowpy)
      - [ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®æ¦‚è¦](#ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®æ¦‚è¦)
      - [å¼•æ•°ã®è§£èª¬ä¸€è¦§](#å¼•æ•°ã®è§£èª¬ä¸€è¦§)
      - [å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰ã‚µãƒ³ãƒ—ãƒ«](#å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰ã‚µãƒ³ãƒ—ãƒ«)
    - [image\_converter\_wand.py](#image_converter_wandpy)
      - [å¼•æ•°](#å¼•æ•°)
      - [å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰ã‚µãƒ³ãƒ—ãƒ«](#å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰ã‚µãƒ³ãƒ—ãƒ«-1)
    - [fire\_organizer.py](#fire_organizerpy)
      - [ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®æ¦‚è¦](#ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®æ¦‚è¦-1)
      - [å¼•æ•°ã®è§£èª¬ä¸€è¦§](#å¼•æ•°ã®è§£èª¬ä¸€è¦§-1)
      - [å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰ã‚µãƒ³ãƒ—ãƒ«](#å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰ã‚µãƒ³ãƒ—ãƒ«-2)
    - [downloader\_danbooru.py](#downloader_danboorupy)
      - [å¼•æ•°ã®è§£èª¬ä¸€è¦§](#å¼•æ•°ã®è§£èª¬ä¸€è¦§-2)
        - [å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰ã‚µãƒ³ãƒ—ãƒ«](#å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰ã‚µãƒ³ãƒ—ãƒ«-3)
    - [downloader\_e621.py](#downloader_e621py)
      - [ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®æ¦‚è¦](#ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®æ¦‚è¦-2)
      - [å¼•æ•°ã®è§£èª¬ä¸€è¦§](#å¼•æ•°ã®è§£èª¬ä¸€è¦§-3)
      - [å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰ã‚µãƒ³ãƒ—ãƒ«](#å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰ã‚µãƒ³ãƒ—ãƒ«-4)
    - [fancaps\_episode\_downloader.py](#fancaps_episode_downloaderpy)
      - [ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®æ¦‚è¦](#ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®æ¦‚è¦-3)
      - [å¼•æ•°ã®è§£èª¬ä¸€è¦§](#å¼•æ•°ã®è§£èª¬ä¸€è¦§-4)
      - [å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰ã‚µãƒ³ãƒ—ãƒ«](#å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰ã‚µãƒ³ãƒ—ãƒ«-5)
    - [image\_cleaner.py](#image_cleanerpy)
      - [æ¦‚è¦](#æ¦‚è¦-1)
      - [å¼•æ•°ä¸€è¦§](#å¼•æ•°ä¸€è¦§)
      - [å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰ã‚µãƒ³ãƒ—ãƒ«](#å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰ã‚µãƒ³ãƒ—ãƒ«-6)
      - [ã—ãã„å€¤ã®ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³](#ã—ãã„å€¤ã®ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³)
    - [Danbooruãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ³ãƒãƒ¼ã‚¿ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼šmetadata\_converter\_danbooru.py](#danbooruãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ³ãƒãƒ¼ã‚¿ã‚¹ã‚¯ãƒªãƒ—ãƒˆmetadata_converter_danboorupy)
      - [æ¦‚è¦](#æ¦‚è¦-2)
      - [å¼•æ•°ä¸€è¦§](#å¼•æ•°ä¸€è¦§-1)
      - [ä»•æ§˜](#ä»•æ§˜)
      - [å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰ã‚µãƒ³ãƒ—ãƒ«](#å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰ã‚µãƒ³ãƒ—ãƒ«-7)
    - [EXIFãƒªãƒ ãƒ¼ãƒãƒ¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆ: exif\_remover.py](#exifãƒªãƒ ãƒ¼ãƒãƒ¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆ-exif_removerpy)
      - [æ¦‚è¦](#æ¦‚è¦-3)
      - [å¼•æ•°ä¸€è¦§](#å¼•æ•°ä¸€è¦§-2)
      - [å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰ã‚µãƒ³ãƒ—ãƒ«](#å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰ã‚µãƒ³ãƒ—ãƒ«-8)
  - [ã‚³ãƒ³ãƒˆãƒªãƒ“ãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³](#ã‚³ãƒ³ãƒˆãƒªãƒ“ãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³)
  - [ãƒ©ã‚¤ã‚»ãƒ³ã‚¹](#ãƒ©ã‚¤ã‚»ãƒ³ã‚¹)
  - [ä½œè€…](#ä½œè€…)
- [â–¼English](#english)
- [data-kitchen ğŸ³](#data-kitchen-)
  - [Overview](#overview)
  - [About the Program](#about-the-program)
  - [Main Features](#main-features)
  - [Installation](#installation)
    - [Required Environment](#required-environment)
    - [Installation Steps](#installation-steps)
  - [Usage](#usage)
    - [image\_converter\_pillow.py](#image_converter_pillowpy-1)
      - [Script Overview](#script-overview)
      - [Explanation of Arguments](#explanation-of-arguments)
      - [Sample Execution Command](#sample-execution-command)
    - [image\_converter\_wand.py](#image_converter_wandpy-1)
      - [Arguments](#arguments)
      - [Sample Execution Command](#sample-execution-command-1)
    - [fire\_organizer.py](#fire_organizerpy-1)
      - [Script Overview](#script-overview-1)
      - [Explanation of Arguments](#explanation-of-arguments-1)
      - [Sample Execution Command](#sample-execution-command-2)
    - [downloader\_danbooru.py](#downloader_danboorupy-1)
      - [Explanation of Arguments](#explanation-of-arguments-2)
        - [Sample Execution Command](#sample-execution-command-3)
    - [downloader\_e621.py](#downloader_e621py-1)
      - [Script Overview](#script-overview-2)
      - [Explanation of Arguments](#explanation-of-arguments-3)
      - [Sample Execution Command](#sample-execution-command-4)
    - [fancaps\_episode\_downloader.py](#fancaps_episode_downloaderpy-1)
      - [Script Overview](#script-overview-3)
      - [Explanation of Arguments](#explanation-of-arguments-4)
      - [Sample Execution Command](#sample-execution-command-5)
    - [image\_cleaner.py](#image_cleanerpy-1)
      - [Overview](#overview-1)
      - [Argument List](#argument-list)
      - [Sample Execution Command](#sample-execution-command-6)
      - [Threshold Guidelines](#threshold-guidelines)
    - [Danbooru Metadata Converter Script: metadata\_converter\_danbooru.py](#danbooru-metadata-converter-script-metadata_converter_danboorupy)
      - [Overview](#overview-2)
      - [Argument List](#argument-list-1)
      - [Specifications](#specifications)
      - [Sample Execution Command](#sample-execution-command-7)
    - [EXIF Remover Script: exif\_remover.py](#exif-remover-script-exif_removerpy)
      - [Overview](#overview-3)
      - [Argument List](#argument-list-2)
      - [Sample Execution Command](#sample-execution-command-8)
  - [Contribution](#contribution)
  - [License](#license)
  - [Author](#author)

# â–¼æ—¥æœ¬èª

# data-kitchenğŸ³

## æ¦‚è¦

ã€Œdata-kitchenã€ã¯ã€åé›†ã—ãŸç”»åƒã‚„ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãªã©ã‚’å‰å‡¦ç†ã™ã‚‹ãŸã‚ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆé›†ã§ã™ã€‚ç”»åƒç”ŸæˆAIãªã©ã®æ©Ÿæ¢°å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆã‚’åŠ¹ç‡åŒ–ã™ã‚‹ãŸã‚ã®ãƒ„ãƒ¼ãƒ«ã‚’æä¾›ã—ã¾ã™ã€‚

![](https://raw.githubusercontent.com/WarriorMama777/imgup/main/img/__Repository/Github/data-kitchen/WebAssets_heroimage_data-kitchen_03_comp001.webp "WarriorMama777/data-kitchen")

## ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã«ã¤ã„ã¦

é‡è¦ï¼šã“ã®ãƒªãƒã‚¸ãƒˆãƒªã®ã™ã¹ã¦ã®ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã¯ChatGPT4ç­‰ã«ã‚ˆã£ã¦æ›¸ã‹ã‚Œã¦ã„ã¾ã™ã€‚ç§ã¯ãƒãƒ³ãƒ—ãƒ­ã‚°ãƒ©ãƒãƒ¼ã§ã‚ã‚Šã€1è¡Œã‚‚ã‚³ãƒ¼ãƒ‰ã¯æ›¸ã„ã¦ã„ãªã„ã“ã¨ã‚’ã”äº†æ‰¿ãã ã•ã„ã€‚

## ä¸»ãªæ©Ÿèƒ½

- ç”»åƒã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
- ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã¨å‡¦ç†
- ç”»åƒã®å‰å‡¦ç†ï¼ˆãƒªã‚µã‚¤ã‚ºã€ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã€ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãªã©ï¼‰
- ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä½œæˆã¨ç®¡ç†

## ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

### å¿…è¦ãªç’°å¢ƒ

- Python 3.x

### ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ‰‹é †

1.  ãƒªãƒã‚¸ãƒˆãƒªã‚’ã‚¯ãƒ­ãƒ¼ãƒ³ã—ã¾ã™ã€‚

```cmd
git clone https://github.com/WarriorMama777/data-kitchen.git
```

```
cd data-kitchen
```

2.  (æ¨å¥¨)Pythonã®ä»®æƒ³ç’°å¢ƒã‚’ä½œæˆã—ã¾ã™

```cmd
python -m venv venv
```

3.  Pythonã®ä»®æƒ³ç’°å¢ƒã‚’ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã«ã—ã¾ã™

- Windows

```cmd
.\venv\Scripts\activate
```

- Linux

```
source venv/bin/activate
```

4.  å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã™ã€‚

```cmd
pip install -r requirements.txt
```

5.  å®Œäº†  
    å‡¦ç†ã™ã‚‹ç›®çš„ã«æ²¿ã£ãŸã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’é¸æŠã—ã¦å®Ÿè¡Œã—ã¾ã™ã€‚  
    ä¾‹: tag_editor.py

```
python tag_editor.py --dir .\data --save_dir .\output --extension txt --del_first 5 --add_last _edited
```

## ä½¿ã„æ–¹

### image_converter_pillow.py

#### ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®æ¦‚è¦

pillowã‚’ä½¿ç”¨ã—ãŸç”»åƒå¤‰æ›å‡¦ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§ã™ã€‚

#### å¼•æ•°ã®è§£èª¬ä¸€è¦§

- `--dir`: å¿…é ˆã€‚å¯¾è±¡ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
- `--save_dir`: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ 'output/'
- `--extension`: å¿…é ˆã€‚å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ã®æ‹¡å¼µå­
- `--recursive`: ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å‡¦ç†ã™ã‚‹å ´åˆã¯æŒ‡å®š
- `--background`: é€æ˜ç”»åƒã®èƒŒæ™¯è‰²
- `--resize`: ç”»åƒã®æœ€å¤§ã‚µã‚¤ã‚ºã‚’æŒ‡å®šã—ã¦ãƒªã‚µã‚¤ã‚º
- `--format`: å‡ºåŠ›ç”»åƒã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
- `--quality`: å‡ºåŠ›ç”»åƒã®å“è³ª
- `--comp`: åœ§ç¸®ãƒ¬ãƒ™ãƒ«
- `--debug`: ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰
- `--preserve_own_folder`: å…ƒã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã‚’ä¿æŒ
- `--preserve_structure`: ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã‚’ä¿æŒ
- `--gc_disable`: ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç„¡åŠ¹ã«ã™ã‚‹
- `--by_folder`: ãƒ•ã‚©ãƒ«ãƒ€ã”ã¨ã«å‡¦ç†ã™ã‚‹
- `--mem_cache`: ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹
- `--threads`: ä½¿ç”¨ã™ã‚‹ã‚¹ãƒ¬ãƒƒãƒ‰æ•°
- `--save_only_alphachannel`: ã‚¢ãƒ«ãƒ•ã‚¡ãƒãƒ£ãƒ³ãƒãƒ«ãƒ‡ãƒ¼ã‚¿ã®ã¿ä¿å­˜

#### å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰ã‚µãƒ³ãƒ—ãƒ«

Basic

```bash
python image_converter_pillow.py --dir "/path/to/directory" --extension jpg png --recursive --background FFFFFF --resize 300 --format JPEG --quality 80 --threads 4
```

Exsample  
ä»¥ä¸‹ã¯å®Ÿéš›ã®ã‚³ãƒãƒ³ãƒ‰ä¾‹ã§ã™ã€‚æŒ‡å®šã•ã‚ŒãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆjpgã€pngã€webpï¼‰ã‚’æŒ‡å®šã•ã‚ŒãŸå½¢å¼(webp)ã«å¤‰æ›ã—ã€æŒ‡å®šã•ã‚ŒãŸä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜ã—ã¾ã™ã€‚  
ã¾ãŸã€å¤‰æ›ã•ã‚ŒãŸç”»åƒã®å“è³ªã¯90ã«è¨­å®šã•ã‚Œã€åœ§ç¸®ç‡ã‚’4ã«ã€è§£åƒåº¦ã¯2048ãƒ”ã‚¯ã‚»ãƒ«ã«ãƒªã‚µã‚¤ã‚ºã•ã‚Œã€èƒŒæ™¯è‰²ã¯ffffff(å®Œå…¨ãªç™½èƒŒæ™¯)ã«è¨­å®šã•ã‚Œã¾ã™ã€‚ã•ã‚‰ã«ã€ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç„¡åŠ¹åŒ–ã—ã€20å€‹ã®ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’ä½¿ç”¨ã—ã¦ä¸¦åˆ—ã§å‡¦ç†ã•ã‚Œã‚‹ã‚ˆã†ã«è¨­å®šã—ã¦ã„ã¾ã™ã€‚  
ã¾ãŸã€`--preserve_own_folder`ã«ã‚ˆã£ã¦ã€ä¿å­˜å…ˆã«ã¯å¤‰æ›å…ƒã®å¯¾è±¡ãƒ•ã‚©ãƒ«ãƒ€ã§ã‚ã‚‹"id_990001__1005000"ãŒæ–°è¦ä½œæˆã•ã‚Œã¦ã€ãã“ã«å¤‰æ›ã•ã‚ŒãŸç”»åƒãŒä¿å­˜ã•ã‚Œã¾ã™ã€‚

```bash
python image_converter_pillow.py --dir "H:\Resources\images_by_15000id_02\id_990001__1005000" --save_dir "H:\Resources\webp\images_by_15000id_02" --extension jpg png webp --preserve_own_folder --format webp --quality 90 --comp 4 --resize 2048 --background ffffff --gc_disable --threads 20
```

### image_converter_wand.py

image magick ã®pythonãƒ©ãƒƒãƒ‘ãƒ¼ã§ã‚ã‚‹wandã‚’ä½¿ç”¨ã—ãŸç”»åƒå¤‰æ›å‡¦ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§ã™ã€‚é€šå¸¸ã¯pillowãƒãƒ¼ã‚¸ãƒ§ãƒ³ã§ååˆ†ã§ã€ã‚ãˆã¦ä½¿ã†å¿…è¦ã¯ãªã„ã§ã—ã‚‡ã†ã€‚ä½¿ç”¨ã™ã‚‹ã«ã¯image magickãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ä¾‹ãˆã°chocolateyã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã™ã€‚`choco install imagemagick` (Windows)

#### å¼•æ•°

- `--dir`: å‡¦ç†å¯¾è±¡ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
- `--save_dir`: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
- `--extension`: å‡¦ç†å¯¾è±¡ã¨ãªã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã®æ‹¡å¼µå­
- `--recursive`: ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚‚å«ã‚ã¦æ¢ç´¢
- `--background`: é€éç”»åƒã®èƒŒæ™¯è‰² ä¾‹ï¼š#ffffff
- `--resize`: ãƒªã‚µã‚¤ã‚ºã™ã‚‹é•·è¾ºã®ã‚µã‚¤ã‚º
- `--format`: å¤‰æ›å¾Œã®ç”»åƒå½¢å¼
- `--quality`: ç”»åƒå“è³ª
- `--comp`: ç”»åƒåœ§ç¸®ã®å¼·åº¦
- `--debug`: ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰
- `--preserve_own_folder`: å…ƒã®ãƒ•ã‚©ãƒ«ãƒ€åã‚’ä¿æŒ
- `--preserve_structure`: ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã‚’ä¿æŒ
- `--gc_disable`: ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç„¡åŠ¹åŒ–
- `--by_folder`: ãƒ•ã‚©ãƒ«ãƒ€ã”ã¨ã«å‡¦ç†
- `--mem_cache`: ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ä½¿ç”¨ (ON / OFF)
- `--threads`: ä½¿ç”¨ã™ã‚‹ã‚¹ãƒ¬ãƒƒãƒ‰æ•°

#### å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰ã‚µãƒ³ãƒ—ãƒ«

```bash
python image_converter_wand.py --dir input_images/ --save_dir "./output_images" --extension jpg jpeg png --recursive --resize 800 --format jpeg --quality 80 --comp 6 --debug
```

### fire_organizer.py

#### ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®æ¦‚è¦

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ã€æŒ‡å®šã—ãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ•´é “ã™ã‚‹ãŸã‚ã®ã‚‚ã®ã§ã™ã€‚

#### å¼•æ•°ã®è§£èª¬ä¸€è¦§

- --copy: ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚³ãƒ”ãƒ¼ã—ã¾ã™
- --cut: ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ‡ã‚Šå–ã‚Šã¾ã™
- --dir: å‡¦ç†å¯¾è±¡ã¨ãªã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
- --extensions: å‡¦ç†å¯¾è±¡ã¨ãªã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã®æ‹¡å¼µå­
- --file_name: å‡¦ç†å¯¾è±¡ã¨ãªã‚‹ãƒ•ã‚¡ã‚¤ãƒ«å
- --save: å‡¦ç†å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã™ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
- --preserve_structure: ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®æ§‹é€ ã‚’ä¿æŒã—ã¦ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã—ã¾ã™
- --preserve_own_folder: `--dir`ã§æŒ‡å®šã•ã‚ŒãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªè‡ªä½“ã®ãƒ•ã‚©ãƒ«ãƒ€ã‚’`--save`ã®å ´æ‰€ã«ä½œæˆã—ã¾ã™
- --debug: ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’è¡¨ç¤ºã—ã¾ã™
- --processes: ä½¿ç”¨ã™ã‚‹ãƒ—ãƒ­ã‚»ã‚¹æ•°
- --multi_threading: ãƒãƒ«ãƒã‚¹ãƒ¬ãƒƒãƒ‰å‡¦ç†ã‚’æœ‰åŠ¹ã«ã—ã¾ã™
- --gc-disable: ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç„¡åŠ¹ã«ã—ã¾ã™

#### å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰ã‚µãƒ³ãƒ—ãƒ«

```python
python fire_organizer.py --copy --dir "/input/directory" --extensions .txt --save "/output/directory" --preserve_structure --processes 4
```

### downloader_danbooru.py

â€»gallery-dlã‚’ä½¿ã£ãŸã»ã†ãŒæ—©ã„ã®ã§ä½¿ã†å¿…è¦ã¯ã»ã¨ã‚“ã©ã‚ã‚Šã¾ã›ã‚“ã€‚  
ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ã€æŒ‡å®šã—ãŸã‚¿ã‚°ã§Danbooruã‹ã‚‰ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã™ã‚‹ãŸã‚ã®ã‚‚ã®ã§ã™ã€‚

#### å¼•æ•°ã®è§£èª¬ä¸€è¦§

- --tags: ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹éš›ã«æ¤œç´¢ã™ã‚‹ã‚¿ã‚°ã‚’æŒ‡å®šã—ã¾ã™ã€‚
- --output: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æŒ‡å®šã—ã¾ã™ã€‚(ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: output/)
- --url: Danbooruã®APIã‚³ãƒ¼ãƒ«ã‚’è¡Œã†ãŸã‚ã®URLã‚’æŒ‡å®šã—ã¾ã™ã€‚(ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: https://danbooru.donmai.us)
- --page_limit: ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ™‚ã«è§£æã™ã‚‹æœ€å¤§ãƒšãƒ¼ã‚¸æ•°ã‚’æŒ‡å®šã—ã¾ã™ã€‚(ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1000)
- --api_key: Danbooruã®APIã‚­ãƒ¼ã‚’æŒ‡å®šã—ã¾ã™ã€‚é«˜ãƒ¬ãƒ™ãƒ«ã®ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚’ãƒªãƒ³ã‚¯ã—ã¦åˆ¶é™ã‚’è¶…ãˆã‚‹å ´åˆã«ã®ã¿å¿…è¦ã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼åã‚‚æŒ‡å®šã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
- --username: Danbooruã«ãƒ­ã‚°ã‚¤ãƒ³ã™ã‚‹ãŸã‚ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼åã‚’æŒ‡å®šã—ã¾ã™ã€‚api_keyã¨å…±ã«æŒ‡å®šã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
- --max_file_size: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã§ã¯ãªãã€æœ€å¤§åˆ©ç”¨å¯èƒ½ãªãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã‚ˆã†ã¨ã—ã¾ã™ã€‚
- --extensions: ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã®æ‹¡å¼µå­ã‚’æŒ‡å®šã—ã¾ã™ã€‚ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§è¤‡æ•°æŒ‡å®šã§ãã¾ã™ã€‚(ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: .png,.jpg)
- --save_tags: å„ç”»åƒã®ã‚¿ã‚°ã‚’åŒã˜åå‰ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã¾ã™ã€‚
- --tags_only: ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã›ãšã€å­˜åœ¨ã™ã‚‹ç”»åƒã®ã‚¿ã‚°ã®ã¿ã‚’ä¿å­˜ã—ã¾ã™ã€‚
- --write_translation: ç”»åƒå†…ã®å¤–å›½èªã®ç¿»è¨³ã‚’ã‚¿ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã¿ã¾ã™ã€‚
- --year_start: å†…å®¹ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ãŸã‚ã®é–‹å§‹å¹´ã‚’æŒ‡å®šã—ã¾ã™ã€‚ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ: YYYY
- --year_end: å†…å®¹ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ãŸã‚ã®çµ‚äº†å¹´ã‚’æŒ‡å®šã—ã¾ã™ã€‚ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ: YYYY

##### å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰ã‚µãƒ³ãƒ—ãƒ«

```bash
python downloader_danbooru.py --tags "cat" --output "./downloaded_images" --page_limit 500 --api_key "api_key_here" --username "username_here" --extensions ".png,.jpg" --save_tags --year_start 2020 --year_end 2022
```

### downloader_e621.py

#### ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®æ¦‚è¦

â€»gallery-dlã‚’ä½¿ã£ãŸã»ã†ãŒæ—©ã„ã®ã§ä½¿ã†å¿…è¦ã¯ã»ã¨ã‚“ã©ã‚ã‚Šã¾ã›ã‚“ã€‚  
ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ã€e621ã‹ã‚‰æŒ‡å®šã—ãŸã‚¿ã‚°ã‚„æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«é–¢é€£ã™ã‚‹ç”»åƒã‚’å–å¾—ã—ã€ãã‚Œã‚‰ã®ç”»åƒã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã™ã‚‹ãŸã‚ã®ã‚‚ã®ã§ã™ã€‚

#### å¼•æ•°ã®è§£èª¬ä¸€è¦§

- ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®ã‚¿ã‚°ã¾ãŸã¯æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚
- å–å¾—ã™ã‚‹ãƒšãƒ¼ã‚¸æ•°ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚
- ç”»åƒã‚’ä¿å­˜ã™ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚

#### å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰ã‚µãƒ³ãƒ—ãƒ«

```python
python downloader_e621.py
Enter the target tag or search keyword: cat
Enter the number of pages to retrieve: 3
Enter the name of the directory to save images: cat_images
```

### fancaps_episode_downloader.py

#### ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®æ¦‚è¦

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ã€fancapsã‚µã‚¤ãƒˆã‹ã‚‰ã€æŒ‡å®šã•ã‚ŒãŸã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰URLã«å«ã¾ã‚Œã‚‹ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆç”»åƒã‚’ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°ã—ã¦ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ãŸã‚ã®ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚  
æ¬¡ã®ãƒªãƒã‚¸ãƒˆãƒªã®ã‚³ãƒ¼ãƒ‰ã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ã¦æ”¹è‰¯ã•ã›ã¦ã„ãŸã ãã¾ã—ãŸï¼š[m-patino/fancaps-downloader: Fancaps Downloader](https://github.com/m-patino/fancaps-downloader)

#### å¼•æ•°ã®è§£èª¬ä¸€è¦§

- url: ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’é–‹å§‹ã™ã‚‹URL
- --output: ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å…ˆã®ãƒ•ã‚©ãƒ«ãƒ€ã®ãƒ‘ã‚¹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯"Downloads"ï¼‰

#### å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰ã‚µãƒ³ãƒ—ãƒ«

```python
python fancaps_episode_downloader.py "https://fancaps.net/anime/episodeimages.php?21914-hack_Roots/Episode_1" --output "./Downloads"
```

### image_cleaner.py

#### æ¦‚è¦

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ã€æŒ‡å®šã•ã‚ŒãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã—ã€é‡è¤‡ã—ãŸç”»åƒã‚’æ¤œå‡ºã—ã¦æ•´ç†ã—ã¾ã™ã€‚

#### å¼•æ•°ä¸€è¦§

- `--dir`: å‡¦ç†ã™ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆå¿…é ˆï¼‰
- `--save_dir`: é‡è¤‡ã—ã¦ã„ãªã„ç”»åƒã‚’ä¿å­˜ã™ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: `output/`ï¼‰
- `--save_dir_duplicate`: é‡è¤‡ã—ãŸç”»åƒã‚’ä¿å­˜ã™ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
- `--extension`: å‡¦ç†ã™ã‚‹ç”»åƒã®æ‹¡å¼µå­ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: `jpg png webp`ï¼‰
- `--recursive`: ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å†å¸°çš„ã«æ¤œç´¢ã™ã‚‹ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
- `--debug`: ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
- `--threshold`: é‡è¤‡åˆ¤å®šã®ãŸã‚ã®ãƒãƒŸãƒ³ã‚°è·é›¢é–¾å€¤ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 10ï¼‰
- `--preserve_own_folder`: è‡ªèº«ã®ãƒ•ã‚©ãƒ«ãƒ€æ§‹é€ ã‚’ä¿æŒã™ã‚‹ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
- `--preserve_structure`: ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã‚’ä¿æŒã™ã‚‹ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
- `--gc_disable`: ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç„¡åŠ¹ã«ã™ã‚‹ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
- `--by_folder`: ãƒ•ã‚©ãƒ«ãƒ€å˜ä½ã§å‡¦ç†ã™ã‚‹ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
- `--process_group`: å‡¦ç†ã‚°ãƒ«ãƒ¼ãƒ—ã®ç”»åƒæ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 2ï¼‰
- `--mem_cache`: ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æœ‰åŠ¹ã«ã™ã‚‹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: æœ‰åŠ¹ï¼‰

#### å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰ã‚µãƒ³ãƒ—ãƒ«

```python
python image_cleaner.py --dir "/path/to/images" --save_dir "/path/to/output" --threshold 5 --recursive --preserve_structure
```

#### ã—ãã„å€¤ã®ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³

dhashï¼ˆå·®åˆ†ãƒãƒƒã‚·ãƒ¥ï¼‰ã‚’ä½¿ç”¨ã—ã¦ç”»åƒã®é¡ä¼¼æ€§ã‚’åˆ¤å®šã™ã‚‹éš›ã®é©åˆ‡ãªã—ãã„å€¤ï¼ˆHammingè·é›¢ã®é–¾å€¤ï¼‰ã¯ã€å…·ä½“çš„ãªç”¨é€”ã‚„ç”»åƒã®æ€§è³ªã«ä¾å­˜ã—ã¾ã™ãŒã€ä¸€èˆ¬çš„ã«ã¯ä»¥ä¸‹ã®ã‚ˆã†ãªã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã‚’å‚è€ƒã«ã—ã¦ã¿ã¦ãã ã•ã„ã€‚

- **å³å¯†ãªä¸€è‡´ã‚’æ±‚ã‚ã‚‹å ´åˆï¼ˆåŒä¸€ç”»åƒã®æ¤œå‡ºï¼‰**:
    
    - ã—ãã„å€¤: **5ä»¥ä¸‹**
    - ã“ã®è¨­å®šã¯ã€ã»ã¼åŒä¸€ã®ç”»åƒï¼ˆã”ãã‚ãšã‹ãªé•ã„ã®ã¿ï¼‰ã‚’é‡è¤‡ã¨ã—ã¦æ¤œå‡ºã—ã¾ã™ã€‚
- **ä¸€èˆ¬çš„ãªé¡ä¼¼æ€§ã®æ¤œå‡º**:
    
    - ã—ãã„å€¤: **10å‰å¾Œ**
    - ã“ã®ç¯„å›²ã¯ã€ã»ã¨ã‚“ã©åŒã˜ç”»åƒï¼ˆè‰²åˆã„ã‚„å°ã•ãªå¤‰æ›´ã‚’å«ã‚€ï¼‰ã‚’æ¤œå‡ºã™ã‚‹ã®ã«é©ã—ã¦ã„ã¾ã™ã€‚å¤šãã®ç”¨é€”ã§ã“ã‚ŒãŒãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã—ãã„å€¤ã¨ã—ã¦ä½¿ã‚ã‚Œã¾ã™ã€‚
- **ã‚ˆã‚Šç·©ã‚„ã‹ãªé¡ä¼¼æ€§ã®æ¤œå‡º**:
    
    - ã—ãã„å€¤: **15ä»¥ä¸Š**
    - ã“ã‚Œã¯ã€ã‹ãªã‚Šç•°ãªã‚‹ç”»åƒã§ã‚‚é¡ä¼¼æ€§ãŒã‚ã‚‹ã¨è¦‹ãªã—ãŸã„å ´åˆã«ä½¿ç”¨ã—ã¾ã™ã€‚ãŸã ã—ã€ã“ã®è¨­å®šã§ã¯èª¤æ¤œå‡ºï¼ˆæœ¬æ¥ç•°ãªã‚‹ç”»åƒã‚’é‡è¤‡ã¨åˆ¤å®šã™ã‚‹ï¼‰ã®å¯èƒ½æ€§ãŒå¢—åŠ ã—ã¾ã™ã€‚

### Danbooruãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ³ãƒãƒ¼ã‚¿ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼šmetadata_converter_danbooru.py

#### æ¦‚è¦

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ã€Danbooruã®jsonãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ—ãƒ¬ãƒ¼ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ã«å¤‰æ›ã—ã¾ã™ã€‚

#### å¼•æ•°ä¸€è¦§

- `--dir`: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒå«ã¾ã‚Œã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆå¿…é ˆï¼‰
- `--save_dir`: å¤‰æ›ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã™ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆå¿…é ˆï¼‰
- `--metadata_order`: æŠ½å‡ºã™ã‚‹ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ©ãƒ™ãƒ«ã®é †åºã€‚ä¾‹: `--metadata_order "title" "artist" "tags"`ï¼ˆå¿…é ˆï¼‰
- `--insert_custom_text`: å‡ºåŠ›ã«ã‚«ã‚¹ã‚¿ãƒ ãƒ†ã‚­ã‚¹ãƒˆã‚’æŒ‡å®šã•ã‚ŒãŸã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«æŒ¿å…¥ã™ã‚‹ã€‚ä¾‹: `--insert_custom_text 2 "CUSTOM_TEXT"`ï¼ˆä»»æ„ï¼‰
- `--debug`: ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ã‚’æœ‰åŠ¹ã«ã—ã¦ã€å®Ÿéš›ã®å¤‰æ›´ã‚’è¡Œã‚ãšã«å‡¦ç†ãƒ­ã‚°ã‚’è¡¨ç¤ºã™ã‚‹ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
- `--save_extension`: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã®æ‹¡å¼µå­ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: `txt`ï¼‰ï¼ˆä»»æ„ï¼‰
- `--mem_cache`: ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æœ‰åŠ¹ã¾ãŸã¯ç„¡åŠ¹ã«ã™ã‚‹ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯`ON`ï¼ˆä»»æ„ï¼‰
- `--threads`: ä½¿ç”¨ã™ã‚‹ã‚¹ãƒ¬ãƒƒãƒ‰æ•°ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯CPUã‚³ã‚¢æ•°ï¼ˆä»»æ„ï¼‰
- `--recursive`: ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å†å¸°çš„ã«å‡¦ç†ã™ã‚‹ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
- `--preserve_own_folder`: ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã«è‡ªèº«ã®ãƒ•ã‚©ãƒ«ãƒ€æ§‹é€ ã‚’ä¿æŒã™ã‚‹ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
- `--preserve_structure`: ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã‚’ä¿æŒã™ã‚‹ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
- `--gc_disable`: ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç„¡åŠ¹ã«ã™ã‚‹ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
- `--by_folder`: å„ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä¸€ã¤ãšã¤å‡¦ç†ã™ã‚‹ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

#### ä»•æ§˜

- `--dir`ã§æŒ‡å®šã™ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚ã‚‹jsonãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã€æ¬¡ã®ã‚ˆã†ãªå½¢å¼ã§è¨˜è¿°ã•ã‚Œã¦ã„ã‚‹ã“ã¨ãŒæœŸå¾…ã•ã‚Œã¾ã™ã€‚ã“ã‚Œã¯åŸºæœ¬çš„ã«ã¯ã€gallery-dlã®`--write-metadata`å¼•æ•°ã‚’æ¸¡ã—ã¦ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã¨å¾—ã‚‰ã‚Œã¾ã™ã€‚ï¼š[json metadata example](https://github.com/WarriorMama777/data-kitchen/blob/main/example/metadata/danbooru_1_d34e4cf0a437a5d65f8e82b7bcd02606.json)

#### å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰ã‚µãƒ³ãƒ—ãƒ«

```python
python metadata_converter_danbooru.py --metadata_order "tag_string_artist" "tag_string_copyright" "tag_string_character" "tag_string_general" "rating" --dir "/path/to/metadata" --save_dir "/path/to/output" --insert_custom_text 3 "illustration,|||" 4 "|||" --recursive --preserve_own_folder --preserve_structure --gc_disable --by_folder
```

### EXIFãƒªãƒ ãƒ¼ãƒãƒ¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆ: exif_remover.py

#### æ¦‚è¦

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ã€ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰EXIFãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤ã—ã¾ã™ã€‚å†…éƒ¨ã§ä½¿ç”¨ã•ã‚Œã‚‹ä¸»ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™ï¼š

- `cv2`: ç”»åƒã®èª­ã¿è¾¼ã¿ãŠã‚ˆã³ä¿å­˜

#### å¼•æ•°ä¸€è¦§

- `--dir`: å¯¾è±¡ã¨ã™ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆå¿…é ˆï¼‰
- `--remove`: EXIFã‚’å‰Šé™¤ã™ã‚‹ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
- `--save`: ä¿å­˜ã™ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã€‚æŒ‡å®šã—ãªã„å ´åˆã€ç”»åƒã¯ä¸Šæ›¸ãã•ã‚Œã‚‹ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
- `--cpu`: ä½¿ç”¨ã™ã‚‹ã‚¹ãƒ¬ãƒƒãƒ‰æ•°ã€‚æŒ‡å®šã—ãªã„å ´åˆã€è‡ªå‹•çš„ã«æ±ºå®šã•ã‚Œã‚‹ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

#### å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰ã‚µãƒ³ãƒ—ãƒ«

```bash
python exif_remover.py --dir "/path/to/images" --remove --save "/path/to/output" --cpu 4
```

## ã‚³ãƒ³ãƒˆãƒªãƒ“ãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³

è²¢çŒ®ã‚’æ­“è¿ã—ã¾ã™ã€‚ãƒã‚°å ±å‘Šã‚„æ©Ÿèƒ½ææ¡ˆã¯Issueã«ã¦ãŠé¡˜ã„ã—ã¾ã™ã€‚ãƒ—ãƒ«ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚‚æ­“è¿ã§ã™ã€‚

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯MITãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã®ä¸‹ã§å…¬é–‹ã•ã‚Œã¦ã„ã¾ã™ã€‚

## ä½œè€…

- GitHub: [WarriorMama777](https://github.com/WarriorMama777)
- huggingface: [WarriorMama777](https://huggingface.co/WarriorMama777)

* * *

# â–¼English

# data-kitchen ğŸ³

## Overview

"data-kitchen" is a collection of scripts for pre-processing images and metadata downloaded from image sites such as Danbooru, aimed at improving efficiency in creating machine learning datasets for image generation AI and other purposes.

![](https://raw.githubusercontent.com/WarriorMama777/imgup/main/img/__Repository/Github/data-kitchen/WebAssets_heroimage_data-kitchen_03_comp001.webp "WarriorMama777/data-kitchen")

## About the Program

Important: Please note that all programs in this repository are written by ChatGPT4, and I am a non-programmer who has not written a single line of code.

## Main Features

- Image downloading
- Metadata acquisition and processing
- Image pre-processing (resizing, filtering, annotation, etc.)
- Dataset creation and management

## Installation

### Required Environment

- Python 3.x

### Installation Steps

1.  Clone the repository:

```cmd
git clone https://github.com/WarriorMama777/data-kitchen.git
cd data-kitchen
```

2.  Install the required libraries:

```bash
pip install -r requirements.txt
```

## Usage

### image_converter_pillow.py

#### Script Overview

This is a script for image processing.

#### Explanation of Arguments

- `--dir`: Required. Target directory
- `--save_dir`: Output directory. Default is 'output/'
- `--extension`: Required. Target file extensions
- `--recursive`: Process subdirectories if specified
- `--background`: Background color for transparent images
- `--resize`: Resize images to have this max dimension
- `--format`: Output image format
- `--quality`: Output image quality
- `--comp`: Compression level
- `--debug`: Debug mode
- `--preserve_own_folder`: Preserve original directory structure
- `--preserve_structure`: Preserve directory structure
- `--gc_disable`: Disable garbage collection
- `--by_folder`: Process folders one by one
- `--mem_cache`: Use memory cache
- `--threads`: Number of threads to use
- `--save_only_alphachannel`: Save only alpha channel data

#### Sample Execution Command

- Basic

```bash
python image_converter_pillow.py --dir "/path/to/directory" --extension jpg png --recursive --background FFFFFF --resize 300 --format JPEG --quality 80 --threads 4
```

- Exsample  
    The following is an actual command example. It converts the image files (jpg, png, webp) in the specified directory to the specified format (webp) and saves them in the specified destination directory.  
    Additionally, the quality of the converted images is set to 90, with a compression rate of 4, a resolution resized to 2048 pixels, and a background color set to ffffff (pure white). Furthermore, garbage collection is disabled, and it is configured to be processed in parallel using 20 threads.  
    Additionally, the --preserve_own_folder flag creates a new folder in the destination directory named after the original source folder "id_990001__1005000", and the converted images are saved there.

```bash
python image_converter_pillow.py --dir "H:\Resources\images_by_15000id_02\id_990001__1005000" --save_dir "H:\Resources\webp\images_by_15000id_02" --extension jpg png webp --preserve_own_folder --format webp --quality 90 --comp 4 --resize 2048 --background ffffff --gc_disable --threads 20
```

### image_converter_wand.py

Image processing script using wand, a python wrapper for image magick. Usually, the pillow version is sufficient, so there is no need to use it.  
To use this, ImageMagick needs to be installed. For example, you can install it with Chocolatey: `choco install imagemagick` (Windows)

#### Arguments

- `--dir`: Target directory for processing
- `--save_dir`: Output directory
- `--extension`: File extensions to be processed
- `--recursive`: Include subdirectories in the search
- `--background`: Background color for transparent images (e.g., #ffffff)
- `--resize`: Size of the longer side for resizing
- `--format`: Format of the converted images
- `--quality`: Image quality
- `--comp`: Compression strength for images
- `--debug`: Debug mode
- `--preserve_own_folder`: Preserve the original folder name
- `--preserve_structure`: Preserve the directory structure
- `--gc_disable`: Disable garbage collection
- `--by_folder`: Process images by folder
- `--mem_cache`: Use memory cache (ON / OFF)
- `--threads`: Number of threads to use

#### Sample Execution Command

```bash
python image_converter_wand.py --dir input_images/ --save_dir "./output_images" --extension jpg jpeg png --recursive --resize 800 --format jpeg --quality 80 --comp 6 --debug
```

### fire_organizer.py

#### Script Overview

This script is used to organize files within a specified directory. It utilizes the following libraries: argparse, pathlib, shutil, tqdm, concurrent.futures, time, and gc.

#### Explanation of Arguments

- --copy: Copy files
- --cut: Cut files
- --dir: Directory to be processed
- --extensions: File extensions to be processed
- --file_name: Name of the file to be processed
- --save: Directory to save processed files
- --preserve_structure: Preserve the directory structure when saving files
- --preserve_own_folder: Create a folder for the specified directory at the location specified by --save
- --debug: Display debug information
- --processes: Number of processes to use
- --multi_threading: Enable multi-threaded processing
- --gc-disable: Disable garbage collection

#### Sample Execution Command

```python
python fire_organizer.py --copy --dir "/input/directory" --extensions .txt --save "/output/directory" --preserve_structure --processes 4
```

### downloader_danbooru.py

**â€» There is almost no need to use this script since using gallery-dl is faster.**  
This script is used to scrape content from Danbooru based on specified tags. It utilizes the following libraries: argparse, pathlib, requests, os, json, datetime, and tqdm.

#### Explanation of Arguments

- --tags: Specify the tags to search for when downloading content.
- --output: Specify the output directory. (default: output/)
- --url: Specify the Danbooru URL to make API calls to. (default: https://danbooru.donmai.us)
- --page_limit: Specify the maximum number of pages to parse through when downloading. (default: 1000)
- --api_key: Specify the API key for Danbooru. This is optional unless you want to link a higher level account to surpass tag search and page limit restrictions. Username must also be provided.
- --username: Specify the username to log on to Danbooru with, to be provided alongside an api_key.
- --max_file_size: Attempt to download the maximum available file size instead of the default size.
- --extensions: Specify the extensions of file types to download, comma-separated. Pass * to download all file types. (default: .png,.jpg)
- --save_tags: Save the tags for each image in a text file with the same name.
- --tags_only: Only save tags for existing images. Do not download any images.
- --write_translation: Write the translation of foreign text in the image to the tag file.
- --year_start: Specify the start year for downloading content. Format: YYYY
- --year_end: Specify the end year for downloading content. Format: YYYY

##### Sample Execution Command

```bash
python downloader_danbooru.py --tags "cat" --output "./downloaded_images" --page_limit 500 --api_key "api_key_here" --username "username_here" --extensions ".png,.jpg" --save_tags --year_start 2020 --year_end 2022
```

### downloader_e621.py

#### Script Overview

**â€» There is almost no need to use this script since using gallery-dl is faster.**  
This script is designed to retrieve and save images and their metadata related to a specified tag or search keyword. It utilizes libraries such as requests, urllib, BeautifulSoup, and json.

#### Explanation of Arguments

- Please enter the target tag or search keyword.
- Enter the number of pages to retrieve.
- Enter the name of the directory to save images.

#### Sample Execution Command

```python
python downloader_e621.py
Enter the target tag or search keyword: cat
Enter the number of pages to retrieve: 3
Enter the name of the directory to save images: cat_images
```

### fancaps_episode_downloader.py

#### Script Overview

This script is a tool for crawling and downloading screenshot images from the specified episode URL on the fancaps site. It was improved based on the code from the following repository: [m-patino/fancaps-downloader: Fancaps Downloader](https://github.com/m-patino/fancaps-downloader).

#### Explanation of Arguments

- url: URL to start the download
- --output: Path of the output folder (default is "Downloads")

#### Sample Execution Command

```python
python fancaps_episode_downloader.py "https://fancaps.net/anime/episodeimages.php?21914-hack_Roots/Episode_1" --output "./Downloads"
```

### image_cleaner.py

#### Overview

This script processes image files in a specified directory, detects and organizes duplicate images.

#### Argument List

- `--dir`: Directory to process (required)
- `--save_dir`: Directory to save non-duplicate images (default: `output/`)
- `--save_dir_duplicate`: Directory to save duplicate images
- `--extension`: Extensions of images to process (default: `jpg png webp`)
- `--recursive`: Recursively search directories (optional)
- `--debug`: Debug mode (optional)
- `--threshold`: Hamming distance threshold for duplicates (default: 10)
- `--preserve_own_folder`: Preserve own folder structure (optional)
- `--preserve_structure`: Preserve directory structure (optional)
- `--gc_disable`: Disable garbage collection (optional)
- `--by_folder`: Process folders one by one (optional)
- `--process_group`: Number of images in a processing group (default: 2)
- `--mem_cache`: Enable in-memory caching (default: enabled)

#### Sample Execution Command

```python
python image_cleaner.py --dir "/path/to/images" --save_dir "/path/to/output" --threshold 5 --recursive --preserve_structure
```

#### Threshold Guidelines

The appropriate threshold for determining image similarity using dhash (difference hash) depends on the specific use case and the nature of the images, but generally, you can refer to the following guidelines:

- **For strict matching (detecting identical images)**:
    
    - Threshold: **5 or below**
    - This setting detects nearly identical images (with very slight differences) as duplicates.
- **For general similarity detection**:
    
    - Threshold: **around 10**
    - This range is suitable for detecting almost identical images (including slight changes in color or small modifications). This is often used as the default threshold for many applications.
- **For looser similarity detection**:
    
    - Threshold: **15 or above**
    - This is used when you want to consider even significantly different images as similar. However, this setting increases the possibility of false positives (detecting genuinely different images as duplicates).

### Danbooru Metadata Converter Script: metadata_converter_danbooru.py

#### Overview

This script converts Danbooru metadata files to plain text format. The main libraries used internally are as follows:

- `argparse`: Command line argument parsing
- `json`: JSON file reading and parsing
- `os`: File and directory operations
- `signal`: Signal handling
- `sys`: System-related operations
- `pathlib`: Path operations
- `tqdm`: Progress bar display
- `time`: Time-related operations
- `multiprocessing`: Parallel processing
- `gc`: Garbage collection

#### Argument List

- `--dir`: Directory containing metadata files (required)
- `--save_dir`: Directory to save converted files (required)
- `--metadata_order`: Order of metadata labels to extract. Example: `--metadata_order "title" "artist" "tags"` (required)
- `--insert_custom_text`: Insert custom texts at specified indexes in the output. Example: `--insert_custom_text 2 "CUSTOM_TEXT"` (optional)
- `--debug`: Enable debug mode to display processing logs without making actual changes (optional)
- `--save_extension`: Extension of the output file (default: `txt`) (optional)
- `--mem_cache`: Enable or disable memory caching. Default is `ON` (optional)
- `--threads`: Number of threads to use. Default is the number of CPU cores (optional)
- `--recursive`: Recursively process directories (optional)
- `--preserve_own_folder`: Preserve own folder structure in the save directory (optional)
- `--preserve_structure`: Preserve directory structure in the save directory (optional)
- `--gc_disable`: Disable garbage collection (optional)
- `--by_folder`: Process each folder one by one (optional)

#### Specifications

- The json metadata files in the directory specified by `--dir` are expected to be in the following format. This is basically obtained by downloading with the `--write-metadata` argument of gallery-dl: [json metadata example](https://github.com/WarriorMama777/data-kitchen/blob/main/example/metadata/danbooru_1_d34e4cf0a437a5d65f8e82b7bcd02606.json)

#### Sample Execution Command

```python
python metadata_converter_danbooru.py --metadata_order "tag_string_artist" "tag_string_copyright" "tag_string_character" "tag_string_general" "rating" --dir "/path/to/metadata" --save_dir "/path/to/output" --insert_custom_text 3 "illustration,|||" 4 "|||" --recursive --preserve_own_folder --preserve_structure --gc_disable --by_folder
```

### EXIF Remover Script: exif_remover.py

#### Overview

This script removes EXIF metadata from image files. The main libraries used internally are as follows:

- `cv2`: Image reading and saving

#### Argument List

- `--dir`: Target directory (required)
- `--remove`: Remove EXIF metadata (optional)
- `--save`: Directory to save the images. If not specified, the images will be overwritten (optional)
- `--cpu`: Number of threads to use. If not specified, it will be determined automatically (optional)

#### Sample Execution Command

```bash
python exif_remover.py --dir "/path/to/images" --remove --save "/path/to/output" --cpu 4
```

## Contribution

Contributions are welcome. Please feel free to report bugs, suggest features, or contribute via pull requests.

## License

This project is licensed under the MIT license.

## Author

- GitHub: [WarriorMama777](https://github.com/WarriorMama777)
- huggingface: [WarriorMama777](https://huggingface.co/WarriorMama777)